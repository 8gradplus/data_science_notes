
@book{hasties,
  address = {New York, NY, USA},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  publisher = {Springer New York Inc.},
  series = {Springer Series in Statistics},
  timestamp = {2008-05-16T16:17:43.000+0200},
  title = {The Elements of Statistical Learning},
  year = 2001
}


@book{bishop,
	author = {Bishop, Christopher M.},
	title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
	year = {2006},
	isbn = {0387310738},
	publisher = {Springer-Verlag},
	address = {Berlin, Heidelberg},
} 

@book{barber,
	author = {Barber, David},
	title = {Bayesian Reasoning and Machine Learning},
	year = {2012},
	isbn = {0521518148, 9780521518147},
	publisher = {Cambridge University Press},
	address = {New York, NY, USA},
} 

@misc{understanding_random_forests,
	Author = {Gilles Louppe},
	Title = {Understanding Random Forests: From Theory to Practice},
	Year = {2014},
	Eprint = {arXiv:1407.7502},
}

@book{quinlan_programs_for_ml,
	author = {Quinlan, J. Ross},
	title = {C4.5: Programs for Machine Learning},
	year = {1993},
	isbn = {1-55860-238-0},
	publisher = {Morgan Kaufmann Publishers Inc.},
	address = {San Francisco, CA, USA},
} 

@book{breiman_random_forest,
	author    = {Leo Breiman and
	J. H. Friedman and
	R. A. Olshen and
	C. J. Stone},
	title     = {Classification and Regression Trees},
	publisher = {Wadsworth},
	year      = {1984},
	isbn      = {0-534-98053-8},
	timestamp = {Thu, 03 Jan 2002 00:00:00 +0100},
	biburl    = {https://dblp.org/rec/bib/books/wa/BreimanFOS84},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{HYAFIL197615,
	title = "Constructing optimal binary decision trees is NP-complete",
	journal = "Information Processing Letters",
	volume = "5",
	number = "1",
	pages = "15 - 17",
	year = "1976",
	issn = "0020-0190",
	doi = "https://doi.org/10.1016/0020-0190(76)90095-8",
	url = "http://www.sciencedirect.com/science/article/pii/0020019076900958",
	author = "Laurent Hyafil and Ronald L. Rivest",
	keywords = "Binary decision trees, computational complexity, NP-complete"
}


@book{Ripley1995,
	author = {Ripley, Brian D. and Hjort, N. L.},
	title = {Pattern Recognition and Neural Networks},
	year = {1995},
	isbn = {0521460867},
	edition = {1st},
	publisher = {Cambridge University Press},
	address = {New York, NY, USA},
} 

@article{BLUMER1987377,
	title = "Occam's Razor",
	journal = "Information Processing Letters",
	volume = "24",
	number = "6",
	pages = "377 - 380",
	year = "1987",
	issn = "0020-0190",
	doi = "https://doi.org/10.1016/0020-0190(87)90114-1",
	url = "http://www.sciencedirect.com/science/article/pii/0020019087901141",
	author = "Anselm Blumer and Andrzej Ehrenfeucht and David Haussler and Manfred K. Warmuth",
	keywords = "Machine learning, induction, inductive inference, Occam's Razor, methodology of science",
	abstract = "We show that a polynomial learning algorithm, as defined by Valiant (1984), is obtained whenever there exists a polynomial-time method of producing, for any sequence of observations, a nearly minimum hypothesis that is consistent with these observations."
}

@BOOK{cart84,
	author        = {L. Breiman and J. Friedman and R. Olshen and C. Stone},
	title         = {{Classification and Regression Trees}},
	publisher     = {Wadsworth and Brooks},
	address       = {Monterey, CA},
	year          = {1984},
	note          = {new edition \cite{cart93}?},
	remarks       = {cited in \cite{cslu:esca98mm, cslu:icslp98cronk, cstr:unitsel97} for CART, clustering, and decision trees},
	abstract      = {},
}

@Article{Breiman1996,
	author="Breiman, Leo",
	title="Technical Note: Some Properties of Splitting Criteria",
	journal="Machine Learning",
	year="1996",
	month="Jul",
	day="01",
	volume="24",
	number="1",
	pages="41--47",
	abstract="Various criteria have been proposed for deciding which split is best at a given node of a binary classification tree. Consider the question: given a goodness-of-split criterion and the class populations of the instances at a node, what distribution of the instances between the two children nodes maximizes the goodness-of-split criterion? The answers reveal an interesting distinction between the gini and entropy criterion.",
	issn="1573-0565",
	doi="10.1023/A:1018094028462",
	url="https://doi.org/10.1023/A:1018094028462"
}

@misc{epub1833,
	series = {sfb386},
	abstract = {The Gini gain is one of the most common variable selection criteria in machine learning. We derive the exact distribution of the maximally selected Gini gain in the context of binary classification using continuous predictors by means of a combinatorial approach. This distribution provides a formal support for variable selection bias in favor of variables with a high amount of missing values when the Gini gain is used as split selection criterion, and we suggest to use the resulting p-value  as an unbiased split selection criterion in recursive partitioning algorithms. We demonstrate the efficiency of our novel method in simulation- and real data- studies from veterinary gynecology in the context of binary classification and continuous predictor variables with different numbers of missing values. Our method is extendible to categorical and ordinal predictor variables and to other split selection criteria such as the cross-entropy criterion.},
	author = {Carolin Strobl and Anne-Laure Boulesteix and Thomas Augustin},
	volume = {464},
	title = {Unbiased split selection for classification trees based on the Gini Index},
	year = {2005},
	url = {http://nbn-resolving.de/urn/resolver.pl?urn=nbn:de:bvb:19-epub-1833-1}
}



@article{gini_vs_twoing,
	author = {Kayri, Murat and Kayri, İsmail},
	year = {2015},
	month = {09},
	pages = {21-30},
	title = {The Comparison of Gini and Twoing Algorithms in Terms of Predictive Ability and Misclassification Cost in Data Mining: An Empirical Study},
	volume = {27},
	journal = {International Journal of Computer Trends and Technology},
	doi = {10.14445/22312803/IJCTT-V27P105}
}


@article{BERZAL200331,
	title = "On the quest for easy-to-understand splitting rules",
	journal = "Data and Knowledge Engineering",
	volume = "44",
	number = "1",
	pages = "31 - 48",
	year = "2003",
	issn = "0169-023X",
	doi = "https://doi.org/10.1016/S0169-023X(02)00062-9",
	url = "http://www.sciencedirect.com/science/article/pii/S0169023X02000629",
	author = "Fernando Berzal and Juan-Carlos Cubero and Fernando Cuenca and Marı́a J. Martı́n-Bautista"
}