\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{amsopn}
\usetikzlibrary{arrows}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\usepackage{standalone}
\usepackage{tikz}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}
\usetikzlibrary{positioning,calc}

\begin{document}
\bibliographystyle{plain}
\title{Neural Networks}
\maketitle

Neural Networks have the remarkable property that they are able to improve their performance with more data. In many other methods more data certainly helps, but the performance of the algorthms tends to achieve a plateau upon a certain amount of data. Furthermore nowadays more complex MM architectures can be run. So NN benefit from enhanced computer power because we are able to 1) handle more data 2)build more complex architectures, and 3) NN particularily benefit from more data. A summary of historic as well as recent developments and achievement of NN can be found in Ref. \cite{DBLP:journals/corr/Schmidhuber14}.

NN are typically used for supervised learning problem. They had quite some success for unstructured data (images, audio). So we can roughly say that there are the following main architectures:
\begin{itemize}	\setlength\itemsep{0em}
\item Standard NN (classification)
\item Convolutional NN (image data)
\item Recurrent NN (time series, sequence data)

\end{itemize}

\section{Notation} 
We start with the notation for a classification. Todo: generalize this
\begin{itemize}	\setlength\itemsep{0em}
\item $m$ Number of training examples
\item $x^i$ $i$-th feature, $x^i \in \mathbb R ^ {n_x}$ 
\item with $n_x$ the number of features
\item $y^i$ $i$-th label, $y^i \in \mathcal Y$
\item $\tilde y^i$ $i$-th predicted label $\tilde y^i \in \mathcal Y$
\item with $\mathcal Y$ the individual target classes.
\item Feature Matrix $\mathbb R^{m \times n_x} \ni M = \left(x^1, \dots x^m\right)  $ 
\item Label vector $\mathbb R ^m \ni Y    = (y^1, \dots y^m)$
\end{itemize}
So the columns of the feature Matrix are the individual training features. Note that in other frameworks the feature matrix is defined as the transpose

\section{Activation Functions}
\paragraph{Non-linear activation functions}
\begin{itemize}
\item Sigmoid function
\begin{align}
	f(x) = \frac{1}{1+e^{-x}}
\end{align}
\item tanh activation function
\begin{align}
	f(x)=\tanh(x)
\end{align}
\item Rectified linear unit (Relu)
\begin{align}
f (x) = \left\{
\begin{array}{ll}
x & x \geq 0 \\
0 &  x < 0  \\
\end{array}
\right. 
\end{align}
\item Leaky Relu
\begin{align}
f (x) = \left\{
\begin{array}{ll}
x & x \geq 0 \\
\alpha x &  x < 0  \\
\end{array}
\right. ~ \alpha \geq 0
\end{align}
\end{itemize}
In practice the sigmoid function is not used, except for output layers in classification tasks.
The tanh function outperforms the sigmoid function, since it symmetrizes the sigmoid function and thus has mean of zero. 
Currently the gold standard is the Relu function, which ameliorates problems with very small derives of the tanh activation function. Since the Relu function is $0$ for $z<0$ and thus gives zero derivatives, the leaky relu function avoids this by setting a very small constant negatve slope for $z<0$. In practice there does not seem to be much of a difference between relu and leaky relu.  

Note that all these activation functions are non-linear.

\paragraph{Non-linear activation functions}
In principle there are also linear activation $a=Mz +b$ functions thinkable. These include the identity function. However note, that these destroy the advantage of hidden layers because the linear combination of linear functions is linear. The only useful usecase is to use e.g. the identity activation in the output layer in a regression problem. 

\paragraph{Output-layer activation functions}
Typically in a deep neural network the Relu activation function is used. Only the output layer plays a special role as it needs to reflect the problem structure. 
\begin{itemize}	\setlength\itemsep{0em}
	\item Binary classification 
	\item Multi-class classification
\end{itemize}

\section{Loss and cost functions}
\begin{definition}[Loss function] Let there be the true label $y \in \mathcal Y$ and the predicted label $\tilde y \in \mathcal Y$ for feature x.
Then a loss function is a map
\begin{align*}
L: \mathcal Y \times \mathcal Y \rightarrow \mathbb R
\end{align*}
\end{definition}
Note that the loss function depends on the problem at hand and should be defined appropriately.
\begin{definition}[Cost function] 
\begin{align*}
J = \frac{1}{m}\sum_{i=1} ^m L (y^i, \tilde{y}^i)  
\end{align*} 
\end{definition}

\paragraph*{Examples}
\begin{itemize}
\item Quadratic loss function: $L(y, \tilde y) = \frac{1}{2}(y-\tilde{y})^2$
\item Logistic regression loss function: $L(y, \tilde y) = - y\ln \tilde y - (1-y)\ln (1 - \tilde y)$
\end{itemize}
Eventually the cost function needs to be minimized. Numerically this is done by using optimizers such as gradient descent. However, these are local optimizers and a \textbf{convex} loss function is essential in order to find the global optimum.

\paragraph*{Logistic regression}
The Logistic Regression is given
\begin{align}
\tilde y &= \sigma(w^T x + b) \\
\sigma(z) &= \frac{1}{1+e^{-x}}, 
\end{align}
where $\sigma$ is the logist function, $x$ the training features, and $w$ the model parameters to be learned. $\tilde y$ is interpreted as the probability of beeing in class 1 (and $1 - \tilde y$ as the probability of beeing in class 0). Summarizing, we get
\begin{align}
p(y=1 | x) &= \tilde y \\
p(y=0 | x) &= 1 - \tilde y,
\end{align} 
which can be expressed more compactly as
\begin{align}
p(y|x) = \tilde y ^y (1-\tilde y)^{1-y}
\end{align}
Taking the negative logartithm of this expression just gives the logistic regression loss function. The negative because just the logarithm would yield an objective function to be maximised and the loss function is a quantity we would like to minimize. The associated cost function may be obtained by assuming iid of training data. The total probability then $p(\{y_n\}|\{x_n\}) = \prod_n p(y_n|x_n)$. Using the same argument as for the loss function gives the logistic cost function.
\section{Predicting and training}
Once a NN is specified (architecture, definition of units and activation functions), it needs to be trained and eventually used to compute predictions. A NN may be understood as a computational graph. Together with the the concepts of forward and backward propagation training and predicting is done on this computational graph. 
\paragraph{Predicting.}
Forward propagation is used to compute the output of the NN and thus is equivalent to the prediction step.
\paragraph{Training.} Backward propagation is used to compute the gradients. This is the basic ingredient for computing the parameters of a NN, i.e. the training step. This is achieved by minimizing the cost function. Once the gradients are computed by back propagation algorithm \cite{Rumelhart:backpropagation} of the NN corresponds to minimizing the cost function with respect to the parameter space of the NN.
As in the specification step of the NN the gradients need to be computed individually.

\paragraph{Gradient descent.} Gradient descent is a common method to train the NN. Note that the parameters should be \textbf{initialized  randomly} in order to achieve symmetry breaking. Furthermore for these parameters should be chosen to be small in order to avoid too small gradients, which eventually results in slow learning. Strictly speaking this only holds true for activation functions such as the sigmoid or tanh, which exhibit very small gradients for large input values (that is why we want to keep the input values small upon initialization). 

\section{Deep neural Networks}
Deep neural networks have several hidden layers (Figure \ref{fig:deep_neural_net}). The intuition behind this is, that the first layer finds very local details of the input (simple functions of the input). The ensuing layers then add more complex representations of the overall output (compositional representation).
\begin{figure}
	%\includestandalone[width=\textwidth]{deep_net_diagramm}
	\input{deep_net_diagramm}
	\caption{Illustration of a  deep neural network with a single output unit.}
	\label{fig:deep_neural_net}
\end{figure}
There are several motivations behind such a layered architecture:
\begin{itemize} 
\item Universal approximation theorem 	
\item Circuit theory: one may compute functions with a small (i.e., not many hidden units per layer) deep neural network that would require exponentially many hidden units in a shallow NN \cite{2014arXiv1402.1869M}. The infamous example is computing XOR on (all) the features (note that XOR itself with AND and OR functions itself already needs a deep neural network).
\item An exhausitve list of arguments can be found in \cite{Goodfellow:2016:DL} Chap 6.4

\end{itemize}
\paragraph{Definition and Conventions}
\begin{itemize}	\setlength\itemsep{0em}
	\item $W^{[l]}, b^{[l]}$: Weights of layer $l$
	\item $g^{[l]}$: Activation function of layer $l$
	\item $n^{[l]}$: Number of units in layer $l$
\end{itemize}
Note that the structure depicted in the graphical representation refers to a \textbf{single} training example. In implementations we usually work with vectorized formulars.
\paragraph{Parameters vs Hyperparameters.} Parameters are just the weights $W^{[l]}, b^{[l]}$. These are the quantity to be trained. All other quantities (Number of layers $l$, number of units in layer $l$, activation function of layer $l$, learning rate, ...) are called Hyperparameters.  These quantities must be set empirically.
\paragraph{Forward and Backpropagation}
\paragraph{Vectorized version}
\paragraph{Initialization}

\section{Optimizing Neural Networks}
The predictive performance  of a neureal network depends on the hyperparameters and its architecture. First the necessary concepts for judging the predictive power are presented. Based on that, concepts to improve the prediction accuracy are summarized.
\subsection{Train, development and test set.}
It's common practice to split the Data in 3 Sets: Training set, Hold-out crossvalidation (or development) set, and Test set. The Training set is used to train the model. The development set is used to test specific models and based on that develop better models. Once the choice for the best model is made the test set is used to get an \emph{unbiased} estimate of the model performance. If no unbiased estimate on the model is needed sometimes only train and dev sets are used. How to split the data depends on the amount of data:
\begin{itemize}
	\item \textbf{Conventionally} the splitting was made in the order of $70\,\%  - 30 \,\%$  (omitting the test set) or $60\,\% - 20 \,\% - 20\,\%$. This applies $\approx 10^5$ datapoints on order to make sure to have enough test examples.
	\item \textbf{Big data} usecases with $> 10^6$ datapoints typically have splittings in the order $98\,\% - 1 \,\% - 1 \,\%$  because there are still enough samples for the test statistics.
\end{itemize}

Since neural networks need a lot of data some applications use mismatched data distributions between the training set and development and test distributions. E.g., the training data come from webcrawling and the develpment and test sets come from the specific application at hand. However, in this case it is necessary that the development and the test set come from the same distribution.

\subsection{Bias and Variance.}
The \textbf{bias} of a model describes the complexity of the model. A model with high bias has a (too) simple decision boundary (e.g., linear regression). The bias may be assessed by the model performance on the training set. The \textbf{variance} describes the generalization property of model. A model with high variance tends to perform worse for new data (development set). High variance is also called \textbf{over-fitting}.

An example is given in Table \ref{table:bias_variance}. The worst case is model M3 (high bias and high variance). The ideal case is model M4 (low bias and low variance). Note that the model assessment in Table \ref{table:bias_variance} is based on the assumption that the true labeling contained in train and development set are known without error (the Bayesian error is 0\,\%).  A counterexample to this assumption would be blurry images, where even humans make errors in labeling. 
\begin{table}[]\label{table:bias_variance}

	\begin{tabular}{l llll}
		\hline   \hline
		 model &M1 &M2 &M3 &M4 \\
		\hline 
		Training set error [\%]  &1  &15  &15  &0.5 \\
		Development set  error  [\%]       &11 &16  &30  &1 \\
		\hline
		Problem diagnosis	& high variance &  high bias & high bias & low bias \\
							&  &  & high variance & low variance \\
							\hline \hline					 
	\end{tabular}
\caption{Illustration of bias and variance for 4 models M1 ... M4. .}
\end{table}
Bias and variance  are important quantities upon improving the model performance:
\paragraph{Strategies to reduce bias:}
\begin{itemize}
	\setlength\itemsep{0em}
	\item Bigger network (number of layers / units / hyperparameter search) 
	\item Train longer 
	\item Use other optimization algorithm
	\item Different NN architecture
\end{itemize}
\paragraph{Strategies to reduce variance:}
\begin{itemize}
	\setlength\itemsep{0em}
	\item More training data
	\item Regularization
	\item Different NN architecture
\end{itemize}
By iteration through bias reduction and variance reduction one can systematically improve the model performance. Note that in the case of neural networks reducing the bias and reducing the variance can be done relatively independently. This is a further advantage of neural networks. In the 'old days' one needed to find a bias variance trade off as these two quantities could not be improved independently. Probably the main reason is that nowadays there are much more data at hand. Also note that regularization also affects the bias.

The purpose of the test set is to judge the final performance of the algorithm. Ideally development set and test set error are very similar. However, if the test set error is much larger than the training set error then the development set should be increased.

\subsection{Regularization}
Regularization aims to reduce the variance, i.e., reduce generalization errors. There are several regularization strategies for neural networks \cite{Srihari:regularization_talk, Goodfellow:2016:DL}:	

\paragraph{Parameter norm penalties.} This procedure is also called weight decay and follows the same ideas as standard regularization.  Then this regularization scheme adds to the cost function $J(\theta)$  a penalty term
\begin{align}
    \tilde J(\theta)  =  J(\theta) + \lambda \Omega(\theta), 
%	J \mapsto J - \frac{\lambda}{2m} \sum _{l=1}^L \lVert w^{[l]} \rVert _F ^2, 
\end{align}
where $\lambda$ is an additional hyperparameter and $\theta$ are model parameters, which are determined by minimizing the cost function. $\tilde J$ is then used instead of $J$ as objective function in order to determine the modelparmeters within an optimization algorithm. 
In the specific case of a deep neural network a common regularization scheme is
\begin{align}
	\tilde J(\{w^{[l]}, b^{[l]}\}) =  J(\{w^{[l]}, b^{[l]}\}) + \frac{\lambda}{2m} \sum _{i=1}^L \lVert w^{[i]} \rVert _F ^2, 
\end{align}
where $w^{[l]}, b^{[l]}$ are the weights of layer $l$, $\lVert w \rVert _F$ is the Frobenius norm  ($\lVert w \rVert _F ^2 = \sum_{i, j} w_{ij}^2$,) and $m$ is the number of training examples. This procedure is also called weight decay. Note that in this scheme only the $w^{[l]}$ are regularized - probably because there are much more parameters than in the $b^{[l]}$ and the assumption that this leads to a sufficient regularization.

\textit{Remark:} The disadvantage of this approach is the scaling properties are lost. These can be recovered by regularizing not all layers \cite{Srihari:regularization_talk} ?!?	

\paragraph{Dropout.} In dropout regularization nodes and its edges are removed at random for each training example. This introduces a new hyperparamter the "survival" probability of a node.  This probability can be layer dependent. In particular layers with many units that are suspicious to overfitting may be assigned lower survival probabilities than layers with only a few units. In principle dropout can be also used in the input layer. Dropout has been successful in particular in computer vision. Dropout is implemented by masking the activation with a boolean random draw of of a Bernoulli experiment with given survival probability. In practice  "inverse dropout" is used, which additionally divides the activations by the the survival probability.

\textit{Remark:} With dropout the cost function $J$ is no longer well defined. So the convergence behaviour of $J$ as monotonically decreasing function during iteration is lost.

\paragraph{Data augmentation.} Reduces the variance by artificially creating more data via e.g., random rotations, distortions, flips, crops of images. This approach does \textit{not} introduce new hyperparameters.

\paragraph{Early stopping.} This approach tries to reduce over fitting by "tweaking"  the optimization step. Instead of ensuring, that the cost function w.r.t to the training data is minimized, additionally the cost function or error w.r.t the development set is monitored (optimization is still performed with the training set). One then stops at the optimization step where the cost function of the development data is minimized. This approach roots in the observation, that typically the development set error decreases with increasing optimization steps but then starts to increase again. This approach does \textit{not} introduce new hyperparameters.

\paragraph{Further regularization techniques\cite{Srihari:regularization_talk}:}
\begin{itemize}
	\setlength\itemsep{0em}
	\item Parameter tying and sharing
	\item Bagging/ensemble methods
	\item Tangent methods
	\item Adversarial training
\end{itemize}

\section{Optimization algorithms} \label{sec:optimizers}
Given an objective function $J(\theta)$ an optimization algorithm tries to find a local minimizer $\theta ^* = \operatorname{argmin}_\theta J(\theta)$ \cite{NoceWrig06}. To this end an optimization algorithm generates a sequence of iterates $\{\theta_k\}_{k=0} ^\infty$ that (ideally) terminate at the minimizer. Different algorithms differ the process of generating these iterates. $\theta_0$ is often called initialization. For brevity we will use the notation $\nabla J_k \equiv \nabla_\theta J(\theta)\rvert_{\theta_k}$.   
\subsection{Algorithms used for neural networks}
\paragraph{Gradient descent.}
The update in gradient descent is given by
\begin{align} \label{eq:gradient_descent}
	\theta_{k+1} = \theta_k - \alpha \nabla J_k,
\end{align}
where $\alpha$ is called learning rate (hyper parameter).

\paragraph{Exponentially moving average (EMA).} This is not an optimization algorithm itself but a very common ingredient. Exponentially moving average is a comutationally efficient method to approximate moving averages.
Gradient descent may produce oscillating gradients, which induce a reduced learning rate in order to achieve convergence. Many alogrithmic improvements of gradient descent are based on the first and second moment of the gradient. 

For example, in order to soften  oscillations one could use the moving average over the last $m$ computed derivatives, $\bar \nabla J_k := 1/m \sum_{i=k-m} ^ k \nabla J_k$ instead of the current gradient in Equation \ref{eq:gradient_descent}. However, this would require storing $m-1$ additional gradients. Instead,  the moments are approximated by the exponentially moving average.

Assume a series of data $\{q_k\}$. Then the exponentially moving average is defined iteratively as
\begin{align}
\langle  q \rangle ^{\text{EMA}}_0 &= 0\\ 
\langle  q \rangle ^{\text{EMA}}_k &= \frac{1}{1-\beta^k}\left(\beta \langle  q \rangle ^{\text{EMA}}_{k-1}+ (1-\beta) q_k\right) \label{eq:ema}
\end{align}
There is also refined version of exponentially moving average  that includes bias correction,
\begin{align}
\langle  q \rangle ^{\text{cEMA}}_0 &= 0\\ 
\langle  q \rangle ^{\text{cEMA}}_k &= \frac{1}{1-\beta^k}\left(\beta \langle  q \rangle ^{\text{EMA}}_{k-1}+ (1-\beta) q_k\right)
\end{align}

The term $1/\left(1-\beta^k\right)$ is called \textit{bias correction}. It corrects  detoriations for the first few data points.

\paragraph{Gradient descent with momentum.} 
Gradient descent with momentum replaces the gradient by its first moment (expectation value) obtained by the exponentially weighted average \ref{eq:ema}. The update procedure of \ref{eq:gradient_descent} gets then modified to:
\begin{align}
	 \langle  \nabla J \rangle ^{\text{EMA}}_0 &= 0  ~ \text{(initialization)} \\ 
	\langle  \nabla J \rangle ^{\text{EMA}}_k &= \beta \langle  \nabla J \rangle ^{\text{EMA}}_{k-1}+ (1-\beta) \nabla J_k \\
	\theta_{k+1} &= \theta_k - \alpha 	\langle  \nabla J \rangle ^{\text{EMA}}_k. \label{eq:gradient_moments_update}
\end{align}
The term $\langle  \nabla J \rangle ^{\text{EMA}}_{k-1}$ is called momentum, which lends the algorithm it's name.
$\beta$ describes how many past gradients are used and is in principle an other hyper parameter. In practice it is often set to $\beta=0.8$. Gradient descent with momentum converge typically  faster than gradient descent.

\paragraph{RMSprop \cite{Tieleman2012}.} Gradient descent with momentum damps gradient oscillations by using the approximate averages over the previous gradients. RMSprop  ("Root mean square propagation") tries to damp oscillation in a complementary fashion. It weights the gradient by the inverse of the root mean square of the previous gradients  -- again using as exponentially moving averages as approximations to the average. 

\begin{align}
	\langle \nabla J  \odot \nabla J\rangle ^{\text{EMA}}_0 &= 0   ~ \text{(initialization)}\\ 
		\langle \nabla J  \odot \nabla J\rangle^{\text{EMA}}_k &= \beta \langle \nabla J  \odot \nabla J\rangle^{\text{EMA}}_{k -1} + (1-\beta) \nabla J_k  \odot \nabla J_k, \\
	   \theta_{k+1} &= \theta_k - \alpha \nabla J_k \oslash \left(\sqrt{\langle \nabla J  \odot \nabla J\rangle^{\text{EMA}}_k} + \epsilon \right), \label{eq:rms_prop_update} 
\end{align} 
where the square rood is taken elementwise $(\sqrt{A})_{ij} = \sqrt{A_{ij}}$, and  the  elementwise
 (Hadamard) product $\left(A\odot B\right)_{ij} = A_{ij} B_{ij}$ and division  $\left(A\oslash B\right)_{ij} = A_{ij} / B_{ij}$ is used. The matrix $\epsilon$ prevents division by zero and it's components are typically set to $10^{-8}$. The expression $\langle \nabla J  \odot \nabla J\rangle^{\text{EMA}}_k$ is the element wise second moment of the gradient within the EMA approximation. Thus, the RMSprop update \ref{eq:rms_prop_update} scales the gradients by the element wise RMS (root mean square) using EMA. As with gradient descent with momentum, $\beta$ introduces a new hyper paramter but it's typically set to $\beta=0.8$.

\paragraph{Adam \cite{AdamOptimizer2014}.} Adaptive moment estimation (Adam) combines gradient descent with momentum with RMSprop. It takes the averaged gradients as in \ref{eq:gradient_moments_update} and scales them according to RMSprop \ref{eq:rms_prop_update}. However, instead of using EMA the bias corrected EMA is used. Adam then updates the optimization steps according to
\begin{itemize}	\setlength\itemsep{0em}
	\item Initialize $\theta$ and first and second moments of gradients
	\begin{align*}
	\langle  \nabla J \rangle ^{\text{cEMA}}_0 &= 0   \\ 
	\langle \nabla J  \odot \nabla J\rangle ^{\text{cEMA}}_0 &= 0  
	\end{align*}
	\item Update Update moments
	\begin{align*}
	\langle  \nabla J \rangle ^{\text{cEMA}}_k &= \frac{1}{1-\beta_1^k}\left( \beta_1 \langle  \nabla J \rangle ^{\text{EMA}}_{k-1}+ (1-\beta_1) \nabla J_k \right)\\
	\langle \nabla J  \odot \nabla J\rangle^{\text{cEMA}}_k &= \frac{1}{1-\beta_2^k}\left( \beta_2 \langle \nabla J  \odot \nabla J\rangle^{\text{EMA}}_{k -1} + (1-\beta_2) \nabla J_k  \odot \nabla J_k\right) 
	\end{align*}
	\item Update optimization step
	\begin{align}
	\theta_{k+1} = \theta_k - \alpha \langle  \nabla J \rangle ^{\text{cEMA}}_k \oslash \left(\sqrt{\langle \nabla J  \odot \nabla J\rangle^{\text{cEMA}}_k} + \epsilon \right), \label{eq:adam_update} 
	\end{align}
\end{itemize}
Apart from the learning rate $\alpha$ there are in principle three additional hyper parameters: $\beta_1, \beta_2, \epsilon$. In practice these are not tuned but set to the values $\beta_1 = 0.9, \beta_2=0.999, \epsilon=10^{-8}$ \cite{AdamOptimizer2014}.

\subsection{Problems of optimizers for neural networks}
We briefly judge the optimization strategies for neural networks
\begin{itemize}\setlength\itemsep{0em}
	\item Neural networks pose a \textbf{non-convex optimization problem}. However, the algorithms above are modifications of established algorithms for convex optimization problems. So it would be desirable to have non-convex optimization algorithms.
	\item In principle neural networks may have \textbf{many local minima} and the optimization algorithm finds one of them (of course one would like to find the global minimum). This problem has been debated intensively in the community. However, there is an interesting counterargument: in order to have a local minimum $\theta^*$ \textit{all} components of the second derivative at the minimum must be greater than zero $\partial_\theta ^2 J \rvert _{\theta^*} > 0$. Howver, for a very large NN where $\theta$ is high dimensional (several thousand components) this is very unlikely. So extremal points are most probably saddle points, which don't pose a problem to the the above optimizers. This is a consequence, that low dimensional intuition does not necessarily carry over to high dimensional neural networks.
	\item \textbf{Flat regions} of the cost functions may lead to very slow learning. Algorithms like Adam tend to reduce this problem, but it can still be severe.
	\item All algorithms of section  \ref{sec:optimizers} only use the first derivative of the cost function (and it's first and / or second moments). However in the literature the most powerful standard algorithms also use the second derivative (\textbf{Hessian}).
	\item Mind the paradigm: "In practice, it is more important to choose a model family that is easy to optimize than to use a powerful optimization algorithm" \cite{Goodfellow:2016:DL}. 
\end{itemize}
In summary, there seems to be quite some room for improving optimization algorithms \cite{2017arXiv170807827X}. 


\subsection{Speed up the learning process}
Training a NN is optimizing the cost function with respect ot the parameters. In practice this can be a tricky task for several reasons:
\begin{itemize}\setlength\itemsep{0em}
	\item Algorithm specific problems: learning rate
	\item NN architecture specific problems: vanishing or exploding gradients for (very) deep neural networks.
	\item Large training data (slow computations).
\end{itemize}
For these reasons optimization in neural networks is yet not well understood. We summerize some heuristics often used in the community.

\paragraph{Data normalization.} When data are in the same numerical range a larger learning rate can be chosen. 
\paragraph{Weight initialization.} The problem of vanishing or exploding gradients can be partially reduced by carefully initializing the weights. The weights are initialized by drawing from samples from the unit normal distribution, $\mathcal N(1,0)$ followed by a calibration transformation. The latter depends on the activation function but it's main rational is to account for different number of nodes per layer via a variance argument. Popular initialization schemes are:
\begin{itemize}\setlength\itemsep{0em}
	\item Reilu activation: $w^{[l]}_{ij} = \mathcal N(1,0) \sqrt{\frac{2}{n^{[l-1]}}}$
	\item Tanh activation: $w^{[l]}_{ij} = \mathcal N(1,0) \sqrt{\frac{1}{n^{[l-1]}}}$ (Xavier initialization),
\end{itemize}
where $n^{[l-1]}$ is the number of nodes in layer $n-1$. The aim of these initialization schemes is to reduce the tendency of gradients to explode or vanish for very deep neural networks.

\paragraph{Mini-batch gradient descent.} For moderately many training data a vectorized implementation of gradient descent may have an acceptable speed. In this version all training examples are passed into a single optimization step, which is also called batch (gradient descent). However, for many training examples  (larger than $\sim 10^3$) this may become slow. Mini-batch gradient descent splits the data in mini batches of size. Each optimization step then only sees one mini batch (including the labels and cost function).

An \textbf{epoch} is an entire pass through the training set. While batch gradient descent allows only one optimization step per epoch, mini-batch gradient descent allows for batch size over mini-batch size many optimization steps in one epoch. 

The extreme case of mini-batch size one is called \textbf{stochastic gradient descent}. The mini-batch size is another hyper parameter -- in practice typical values are powers of two in the range 64, 128, ... 512.

\paragraph{Learning rate decay} The algorithms in section  \ref{sec:optimizers} use a fixed learning rage $\alpha$. Intuitively, near the optimum the learning rate should be smaller and larger at areas far away from the optimum. There are several heuristics that adapt the learning rate in terms of epochs $n_e$
\begin{itemize}\setlength\itemsep{0em}
	 \item 
	 	  $ \alpha = \frac{\alpha_0}{1+ d n_e}, $ with hyper parameters $\alpha_0$ (initial learning rate) and $d$ (decay rate).
	 \item $ \alpha = \alpha_0\,0.95^{n_e}, $ with hyper parameters $\alpha_0$ (initial learning rate).
	 \item  $ \alpha = \frac{\alpha_0\,d}{\sqrt{n_e}}, $ with hyper parameters with hyper parameters $\alpha_0$ (initial learning rate) and $d$.
	 	  
\end{itemize} 


\section{Convolutional neural networks}
Convolutional neural networks come from computer vision (image classification, object detection, neural style transfer, ...). If a forward deep neural network was used for such tasks it would yield a very high dimensional input space and thus parameter space. As a consequence, there would not be enough data to prevent overfitting\footnote{For example a $1000\times 1000$ pixel input image with 1000 units in the first hidden layer would lead to a $3\cdot10^6 \times 1000$ dimensional weight matrix just for the first hidden layer (the factor of 3 comes from the 3 RGB channels).}.  



\section{Outlook}
\begin{itemize}\setlength\itemsep{0em}
\item Practical tips \cite{2012arXiv1206.5533B, Hinton2012Practical, DBLP:series/lncs/7700} 
\item Interpretation of NN \cite{2017arXiv170607979M, KinSchAlbMueErhKimDae18}
\item (Denoising) autoencoder 
\item Adversarial nets (generative vs. discriminative)
\item Recurrent neural networks (e.g., Hopfield networks)
\item Convolution neural networks 
\item Hyper parameter tunining using Gaussian processes
\item eg VGG http://www.robots.ox.ac.uk/~vgg/practicals/cnn/index.html
\end{itemize}
\bibliographystyle{plain}
\bibliography{paper}
\end{document}