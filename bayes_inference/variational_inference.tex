\section{Variational Inference}
Variational inference \cite{blei2016variational, Beal03variationalalgorithms} is an alternative to MCMC in the Bayes world. 
In essence, the posterior distributions are approximated by optimizing over a family of testfunctions on a scalar bound, the KL diverence. In the special case where we are interested in the point estimate of of model parameters, this gets more structure and leads \cite{Neal:1999:VEA:308574.308679} to the EM algorithm \cite{Dempster77maximumlikelihood, Gupta:2011:TUE:1969852.1969853}(which also gives an estimate for the hidden variables).  \cite{doi:10.1080/01621459.1990.10476213}.

 For many practical applications the mean field approximation over the variational densities is applied which, leads to coordinate ascent mean-field variational inference (CAVI) \cite{Bishop:2006:PRM:1162264}. CAVI can be seen as "message passing" algorithm and thus connects variational inference with graphical models \cite{Winn:2005:VMP:1046920.1088695, wand_mfvb_2011, NIPS2011_4407, minka2005divergence} (implementation https://dotnet.github.io/infer/). 

From a numerical point of view there have been stochastic generalizations in order to deal with large datasets. In order to avoid computing the functional derivations explicitly for a given model analytically an "automatic differentiation variational inference" (ADVI) method has been proposed \cite{advi_2016}. 

\paragraph{The problem.} Assume that we have random variables $X$ of observed data and $Z$ of latent random variables. Inference of the model $p(x, z) = p(x|z)p(z)$ amounts to finding  the conditional probabilities $p(z|x)$. This quantity is also known posterior. Formally this is trivial via Bayes theorem $p(z | x) = p(x, y) / p(x)$. However computing $p(x) = \int p(x, z) dz$ may be computationally intractable (for example, it may lead to very high dimensional numeric integrations for correlated latent variables). 
Cynically, $p(x)$ is just a normalization factor. In physics it is called partition function and in statistics its called evidence. In fact, computing the evidence is one of the most challenging parts of Bayesian methods.

\paragraph{The optimization problem.}  Variational inference tries to approximately determine  $p(z|x)$ by introducing a family of approximate probability densities $q(z|x) \in \mathcal Q$ and then finding that member that approximates the posterior optimally with respect to the KL divergence, 
\begin{align}
	q^*(z) = \underset{q(z)\in \mathcal Q}{\mathrm{arg min}} 
	~ \kl{q(z)}{p(z|x)}, 
\end{align} 
where the shorthand notation $q(z) \equiv q(z|x)$ was introduced.
This transforms an integration problem to an optimization problem. However, the KL divergence of this form is  of no help to the original problem (computing the marginal $p(x)$) as we still need to compute the term $p(z|x)$ which, requires the term $p(x)$. So we need to find another objective function, the evidence lower bound (ELBO), 
\begin{align} \label{def:elbo}
	\mathcal{F}(q):= \langle \log p(x,z) \rangle _{q(z)} - \langle \log q(z) \rangle _{q(z)}
\end{align}
\begin{theorem}\label{thm:elbo_kl}Let there be given a family of probability densities $\mathcal Q$. Then $q^*(z)$ is a minimizer over $\mathcal Q$ of $\kl{q(z)}{p(z|x)}$ iff it is a maximizer of the ELBO $\mathcal{F}(q)$. 
\end{theorem}
\begin{proof}This follows from
	\begin{align*}
		\kl{q(z)}{p(z|x)}   &=  \langle \log q(z) \rangle _{q(z)} - \langle \log p(z | x) \rangle _{q(z)}   \\     
		  &= - \mathcal{F}(q) + \langle \log p(x) \rangle _{q(z)} \\
		  & = - \mathcal{F}(q)  + p(x), 
	\end{align*}
and that $p(x)$ is constant with respect to $q(z), ~ \delta p(x) / \delta q(z) = 0$
\end{proof}
So, first we need to specify a family of variational densities $\mathcal Q$ and then optimize Equation \ref{def:elbo} over $\mathcal Q$. Not that both, the KL and ELBO objective functions implicitly depend on $x$ and so does $q^*(z)$. This just reflects the dependence on the training data.
\paragraph{Discussion.}Turning to the interpretation of variational inference rewrite
\begin{align}
	\mathcal{F}(q) &= \langle \log p(x | z) \rangle _{q(z)} + \langle \log p(z) \rangle _{q(z)}  - \langle \log q(z) \rangle _{q(z)} \nonumber \\ 
	        &= \langle \log p(x | z) \rangle _{q(z)} - \kl{q(z)}{p(z)}.
\end{align}
The density that maximizes the ELBO thus compromises between  i) putting most probability weight on $z$ where the posterior $p(x|z)$ is large and ii) beeing close to the prior $p(z)$. Furthermore if we assume that the $N$ observed data points are iid of the form $x = x_1, \dots, x_N$  the first expression reads $\langle \log p(x | z) \rangle _{q(z)} = \sum _{i = 1} ^N\langle \log p(x_i | z) \rangle _{q(z)}$. Thus, the first term becomes more important for large number of observed data points. But this is just in line with the usual Bayesian behavior.

Furthermore the ELBO bounds the (log) evidence $p(x)$,
\begin{align}
	\log p(x) = \mathcal{F}(q) + \kl{q(z)}{p(z|x)} \geq \mathcal{F}(q)
\end{align}
because $D(\cdot) \geq 0$. This property lends ELBO its name and has been used for model selection under the assumption that the ELBO is a good approximation to the marginal likelihood. However, this approach is not rigorous. So we will just use the ELBO as objective function for the optimization problem (which is rigorous).

The KL-divergence enjoys the property $\kl{q}{p} = 0 \Leftrightarrow q=p$. Assume a maximizer $q^*$ of the ELBO, which is also a minimizer of the KL-divergence. Now, if the KL-divergence  $\kl{q^*(z)}{p(z|l)} = 0$ then $q^*$ is the true posterior. However this need not be the case as the variational family typically suffers from severe restrictions due computational resources. In this case it is not clear how "good" the minimizer of the KL-divergence approximates the posterior \cite{minka2005divergence, Leisink01atighter}.

\subsection{Mean-field approximation}

\begin{definition}[Mean field approximation]\label{def:mean_field}
	A family of pdfs $q_Z(z) \in \mathcal Q$ is called mean field approximation if it is of the form $q_Z(z) = \prod _\ell q_{Z_\ell}(z_\ell)$ with  
	$q_{Z_\ell}(z_\ell)\in\mathcal Q_\ell $ and $Q = \otimes_\ell\mathcal Q_\ell$.
\end{definition}
Note that the definition does not assume how the latent variables are factorized. The case where the latent variables are fully factorized is sometimes called \textit{naive mean field approximation} while the case where  some latent variables are collected into groups is sometimes called \textit{generalized mean field approximation}.
In the following we use the shorthand notation $q_\ell(z_\ell) \equiv q_{Z_\ell}(z_\ell)$
\begin{theorem}\label{thm:mf_elbo}
	Given a mean field approximation of a variational family,  $\prod _\ell q_\ell(z_\ell) \in  \otimes_\ell \mathcal Q_\ell$ with $\mathcal Q_\ell=\{q_\ell(z_\ell)\,|\,q_\ell(z_\ell) \text{ is pdf}\,\}$. Then the maximizer of the ELBO is given by
	\begin{align}
		q^*(z) &= \prod _\ell q^*_\ell(z_\ell), \label{eq: mean_field_joint}\\
		q^*(z_\ell) &= \frac{1}{N_\ell} \exp{\langle \log p(x, z_\ell, z_{-\ell}) \rangle_{q_{z_{-\ell}}}}, \label{eq:mean_field_update}
		%N_\ell &= \int dz_\ell \exp{\langle \log p(x, z_\ell, z_{-\ell}) \rangle_{q_{z_{-\ell}}}}, 
	\end{align}
	where $z=\{z_\ell, z_{-\ell} \}$,  $z_{-\ell} = \{z_1, \dots, \cancel{z_\ell} \dots\}$ and $q_{z_{-\ell}} = \prod _{\i\neq \ell} q_i(z_i)$
\end{theorem}
\begin{proof}
	Taking  the functional derivative of the ELBO under the constraint that all $q_\ell(z_\ell)$ are pdfs, i.e., $\int q_\ell(z_\ell)\,dz_\ell = 1$ and  $q_\ell(z_\ell) \geq 0 ~ \forall z_\ell$ via Lagrangian multipliers gives, 
	\begin{align*}
		&\frac{\delta}{\delta q_k(z_k')} \left\{\mathcal{F}(\prod _{\ell=1}^L q^*_\ell(z_\ell)) - \sum_{\ell=1}^L \lambda_\ell\left(\int q_\ell(z_\ell)\,dz_\ell - 1\right)\right\} \\
		= &\frac{\delta}{\delta q_k(z_k')} \left\{
		\int dz_1\, \dots \int dz_1\, p(x, z_1, \dots, z_L) \prod _{\ell =1}^L q_\ell(z_\ell)
		- \sum_{\ell=1}^L \int dz_\ell\, q_\ell(z_\ell) \log q_\ell(z_\ell)\right\} \\ 
		&- \lambda_k \int \delta (z_k - z_k' )\,dz_k \\
		= & \int\dots\int dz_1\dots \cancel{dz_k} \dots dz_L \, p(x, z_1, \dots, z_k', \dots z_L) \prod _{\ell \neq k} q_\ell(z_\ell)  -\log q_k(z_k') -1 - \lambda_k.
	\end{align*}
Solving for $q_k(z_k')$ and using the notation from above gives
\begin{align}
q_k(z_k') = \frac{\exp\langle \log p(x, z'_k, z_{-k}) \rangle_{q_{z_{-k }}}}{\exp(1+\lambda_k)}
\end{align}
The Lagrangian multiplier and thus the is obtained by normalization, $N_k = \exp(1+\lambda_k) = \int dz_k' \, \exp\langle \log p(x, z'_k, z_{-k}) \rangle_{q_{z_{-k }}}$. Together with the functional from this implies that $q_k$ is indeed a pdf.
\end{proof}
Note that (\ref{eq:mean_field_update}) can be equivalently written in terms of complete conditionals $p(z_\ell | x, z_{-\ell})$,
\begin{align} \label{eq:conditional_cavi_update}
	q^*(z_\ell) &= \frac{1}{N_\ell'} \exp{\langle \log p(z_\ell | x, z_{-\ell}) \rangle_{q_{z_{-\ell}}}}, 
\end{align}
because $\langle \log p(x, z_\ell, z_{-\ell}) \rangle_{q_{z_{-\ell}}} = \langle \log p(z_\ell | x, z_{-\ell}) \rangle_{q_{z_{-\ell}}} - \langle \log p(x, z_{-\ell}) \rangle_{q_{z_{-\ell}}}$ and the second term does not depend on $q_\ell(z_\ell)$ and thus may be absorbed into the normalization factor. 
\begin{itemize}
\item  Theorem \ref{thm:elbo_kl} and \ref{thm:mf_elbo} establish the best approximation to the KL-divergence of the true posterior within a factorized family of variational densities. Recall $q(z)\equiv q(z|x)$ and $z=\{z_1,\dots,z_\ell, \dots\}$
\item Integrating out definition \ref{def:mean_field} shows that the approximation to the marginal posterior corresponds to the variational component, which is also complete conditional. 
\begin{align}
	q(z_\ell | x) \equiv  q(z_\ell) = q_\ell(z_\ell) \\ 
	q(z_\ell | z_{-\ell}, x) = q_\ell(z_\ell) 
\end{align}
\item   From Theorem \ref{thm:mf_elbo},  $q^*_\ell(z_\ell)$  depends on all other  approximate pdfs of random variables. In practice this yields an iterative scheme of computation, the \textit{coordinate ascent mean-field variational inference algorithm} (CAVI)\cite{blei2016variational, Bishop:2006:PRM:1162264}. This fix-point procedure is summarized in Algorithm \ref{algo:cavi}. The short-hand notation that
$q^{(n+1)}_{z_{<\ell}}\cdot q^{(n)}_{z_{>\ell}} \equiv q^{(n+1)}_1 \dots q^{(n+1)}_{\ell-1} q^{(n)}_{\ell} \dots q^{(n)}_{\ell} $ was used to indicate that the updated variational densities are used as soon as they are available \cite{Bishop:2006:PRM:1162264}. 

\item Especially from Eq (\ref{eq:conditional_cavi_update}) the connection between CAVI and the Gibbs sampler in the MCMC approach \cite{doi:10.1080/01621459.1990.10476213, Geman:1984:SRG:2286442.2286617} can be seen. Both use the complete conditional $p(z_\ell | x, z_{-\ell})$ in their update steps.	

\item  In general this formulation is not very useful for  practical computations. Thus  typically further structure is incorporated by using the exponential family as variational family, which leads to much simpler CAVI update equations and allows variational inference to scale to massive data \cite{blei2016variational, Bernardo03thevariational, MAL-001}.
\end{itemize}
 \begin{algorithm}[t] \label{algo:cavi}
 	\SetAlgoLined
 	\textbf{Input:} A model $p(x,z)$ and a dataset $x$ \\
 	\KwResult{Maximizer of the ELBO}
 	\textbf{Initialize:} variational densities $q_\ell(z_\ell), ~ \ell = 1 \dots m$ \\
 	$n = 0$ \\
 	\While{While  the ELBO   is not converged}{
 		\For{$\ell = 1 \dots m $}{
 				$q^{(n+1)}(z_\ell) \propto \exp{\langle \log p(x, z_\ell, z_{-\ell}) \rangle_{q^{(n+1)}_{z_{<\ell}}\cdot q^{(n)}_{z_{>\ell}}}}$
 		}
 	Compute ELBO $\mathcal F(q^{(n+1)})$ from Eq. (\ref{def:elbo})\\
 	$n = n+1$
 	}
 \textbf{return }$q^{(n)}(z) = \prod_{\ell =1}^{m} q^{(n)}_\ell(z_\ell)$
 	\caption{Coordinate ascent variational inference}
 \end{algorithm} 
\paragraph{Variational Bayesian Expectation-Maximization (VB-EM).}So far there was no interpretation on the latent variables. Consider a model with latent variables $z=\{z,\theta\}$, where $\theta$ are model parameters and $z$ are hidden (e.g., local) variables.  The Variational Bayesian Exptectation-Maximisation (VB-EM) \cite{Beal03variationalalgorithms, Bernardo03thevariational}  is just a consequence of Theorem \ref{thm:mf_elbo} together with the mean-field ansatz $q(z,\theta) = q_z(z)q_\theta(\theta)$, 
\begin{align}
q_z(z) &\propto \exp{\langle \log p(z, x, \theta) \rangle_{q_{\theta}}}, \label{eq:vb1-e-step}  \\
q_\theta(\theta) &\propto \exp{\langle \log p(x, z, \theta) \rangle_{q_z}}, \label{eq:vb1-m-step}
\end{align}
which can be equivalently written as
\begin{align}
	q_z(z) &\propto \exp{\langle \log p(z, x | \theta) \rangle_{q_{\theta}}}, \label{eq:vb-e-step}  \\
	q_\theta(\theta) &\propto p(\theta) \exp{\langle \log p(x, z | \theta) \rangle_{q_z}}. \label{eq:vb-m-step}
\end{align}
This follow $p(x,z,\theta) = p(x,z | \theta)p(\theta)$. The residual contribution  $\exp{\langle \log p(\theta) \rangle_{q_{\theta}}}$ may be absorbed in the normalization factor in (\ref{eq:vb-e-step}) and 
$\exp{\langle \log p(\theta) \rangle_{q_{z}}} = p(\theta)$ in (\ref{eq:vb-m-step}). 
  
\subsection{Parametric variational distributions}  
  
\subsection{Point estimates}  
 Very often $\theta$ is the only quantity of interest, either 1) as a marginal posterior pdf, $\int p(\theta, z|x)\,dz$, or 2) by a point-estimate of it (MAP)  $\operatorname{argmax}_\theta \int p(\theta, z|x)\,dz$, or 3) as a maximum likelihood estimate (MLE)  $\operatorname{argmax}_\theta \int p(x, z|\theta)\,dz$.  Clearly, the first case can be approximately obtained by VB-EM or equivalently, the mean-field ansatz $q(z,\theta) = q(z)q(\theta)$ and using the machinery of Theorem \ref{thm:mf_elbo}.
 
  
\paragraph{Expectation-Maximisation}
The aim of the expectation maximisation (EM) algorithm is provide a maximum likelihood estimate (MLE) of the model parameters $\theta$ under hidden random variables $Z$, $\operatorname{argmax}_\theta p(x | \theta)  = \operatorname{argmax}_\theta \int p(x, z | \theta)\,dz$.


\begin{theorem} \label{em:bound}
	Let there be given a variational family $q\in\mathcal Q$. Then 
	\begin{align} \label{em:functional}
			\mathcal{F}(q, \theta) :=  \langle \log p(x,z | \theta) \rangle _{q(z)} - \langle \log q(z) \rangle _{q(z)} 
	\end{align}
	is a lower bound to the log-likelihood $\log p(x|\theta) \geq \mathcal{F}(q, \theta) ~ \forall q $ with equality iff $q(z)=p(z|x,\theta) \in \mathcal Q$.
\end{theorem}
\begin{proof}
	We have 	$\log p(x | \theta) = \mathcal{F}(q, \theta) + \kl{q(z)}{p(z|x, \theta)} \geq \mathcal{F}(q, \theta)$. Equality follows from the property of the KL-divergence  $\kl{q}{p} = 0 \Leftrightarrow q=p$
\end{proof}

Note that the variational densities only depend on the observed data $q(z) \equiv q(z|x)$ (but not on parameter $\theta$). In contrast to the fully Baysian variational methods above we need the bounding property of theorem \ref{em:bound} in order to lend the maximizer of $\mathcal{F}(q, \theta)$ an interpretation as beeing the MLE. 

For a given variational family $\mathcal Q$ we can now formulate the \textit{constraint} (with respect to $\mathcal Q$ ) EM Algorithm \cite{Beal03variationalalgorithms} by optimizing functional (\ref{em:functional}):
\begin{align}
q^*(z) & =  \operatorname{argmax}_q  \mathcal F (q, \theta) \text{~ (E-step)}\\
\theta^* &= \operatorname{argmax}_\theta  \mathcal F (q, \theta)  \text{~ (M-step)},  
\end{align}
where $\operatorname{argmax}_\theta  \mathcal F (q, \theta)  =	\operatorname{argmax}_\theta \langle \log p(x, z | \theta) \rangle_{q}$ because $q(z)$ does not depend on $\theta$.

If the varitional family is sufficiently large such that $=p(z|x,\theta) \in \mathcal Q$ (in other words if the conditional posterior $p(z|x,\theta)$ is computable) the the equality condition of theorem  \ref{em:bound} can be used to arrive at the \textit{unconstraint}, or classical EM algorithm. \cite{Dempster77maximumlikelihood, Neal:1999:VEA:308574.308679} 
\begin{align}
q^*(z) & =  p(z |x, \theta)  \text{~ (E-step)}\\
\theta^* &= 	\operatorname{argmax}_\theta \langle \log p(x, z | \theta) \rangle_{q}   \text{~ (M-step)}. 
\end{align} 
The M-step follows by taking the functional derivative with respect to $q(z)$ under the constraint $\int q(z)\, dz = 1$. While the constraint EM converges to lower bound of the log-likelihood the uncontraint converges to a local maximum of the log-likelihood. Both variants lead to an iterative algorithm (Algorithm \ref{algo:classical_em}).

 \begin{algorithm}[t] \label{algo:classical_em}
    E-step: $q^{(n)}(z) = p(z|x,\theta^{(n-1)})$ \\
    M-step: $\theta^{(n)}(z) = \operatorname{argmax}_\theta \langle \log p(x, z | \theta) \rangle_{q^{(n)}} $
	\caption{Classical EM Algorithm ($n$-th update)}
\end{algorithm} 

Theorem \ref{em:bound} and the (un)constraint EM formulas are only statements on what happens at the maximizer but not \textit{how} the maximizer is computationally approached. This allows to speed up algorithm \ref{algo:classical_em} for large data sets. Consider a case with global model parameters $\theta$ and local hidden random variables $Z = \{Z1,\dots Z_N\}$ together with $N$ observations $X = \{X_1\dots X_N\}$ such that $p(x,z|\theta) = \prod p(x_i,z_i|\theta)$.  Then choose $q(z)= \prod q_i(z_i)$.  If the goal is to learn $\theta$ there is no need to compute the E-step for all datapoints at each iteration. It is enough to perform the E-step for only one datapoint component as this increases $\mathcal F(q, \theta)$ for fixed $\theta$ (algorithm \ref{algo:sparse_classical_em}). Eventually this leads to "sparse" algorithms which have been shown to converge faster \ref{Neal:1999:VEA:308574.308679}.
 \begin{algorithm}[t] \label{algo:sparse_classical_em}
	Choose some data point $i$ to be updated 
	\begin{align*}
		\text{E-step:\quad} q^{(n)}_k(z) &= \begin{cases}
					p(z_i|x_i,\theta^{(n-1)}) ~ k=i \\
					q^{(n)}_k(z)  ~ k\neq i 
	               \end{cases} \\
	               q^{(n)}(z) &=  \prod_k q^{(n)}_k(z) \\
	    \text{M-step:\quad}  \theta^{(n)}(z) & = \operatorname{argmax}_\theta \langle \log p(x, z | \theta) \rangle_{q^{(n)}}
	\end{align*} 
	\caption{Sparse EM Algorithm ($n$-th update)}
\end{algorithm}
\begin{comment}
 
\paragraph{Relation between EM and variational inference.}
Basic idea
\begin{itemize}
	\item Uniform prior, $p(\theta)$. In principle this could be a degenerate pdf (think of $\theta \in \mathbb R$). To this end take $I_k \in \mathbb{R}^n$. Then the uniform distribution
	$p_k(\theta) = 1/V(I_k)$, with $V$ beeing a volume. Now, the posterior becomes
	\begin{align*}
		p_k(\theta | x) = \frac{p(x | \theta) p_k(\theta)}{\int_{\theta'\in I_k} p(x | \theta') p_k(\theta')\, d\theta} = \frac{p(x | \theta) }{\int_{\theta'\in I_k} p(x | \theta')\, d\theta'},
	\end{align*}
	which enjoys beeing independent of $p(I_k)$. Taking the limit $I_k \rightarrow \mathbb{R}^n$ gives, 
	\begin{align*}
		p(\theta | x) = \frac{p(x | \theta)}{\int_{\theta'\in \mathbb{R}^n} p(x | \theta')\, d\theta'}.
	\end{align*}
	This establishes $p(\theta | x) \propto p(x | \theta)$ for a uniform, possibly degenerate prior $p(\theta)$. While the functions differ  the maxima are the same for fixed $x$ (training data), $\operatorname{argmax}_\theta p(x | \theta) = \operatorname{argmax}_\theta p(\theta | x)$, that is the MLE and MAP estimate coincide in this case. 
	
	Setting $p(\theta)$ to the uniform distribution allows to absorb $p(\theta)$ into the normalization constant in (\ref{eq:vb-m-step}) for the further proof.
	\item Use Delta distributions as variational family for parameter $\theta$. 
	\item In order to avoid the 
	 of taking the log of $\delta$, we use the dirac sequence for the proof. We then need to show: 1) the $\theta$ - update needs to remain in that dirac sequence and 2) taking the limit yields indeed the EM algorithm.
	 
	
\end{itemize}
  
 
 	

 
 \section{Todo}
 \begin{itemize}
	 \item Stochastic VI - make it applicable to batches (scaling)
 	\item Conjecture MAP as special choice of variational family ($\delta$)
 	\item Conjecture MLE as special choice of variational family ($\delta$) + uniform prior.
 	\item benefit of further restricting to exponential family
 	\item parametrized pdfs and contingency table
 \end{itemize}
\paragraph{Example: Variational Bayes EM}
\paragraph{Limitations of variational inference}
\begin{itemize}
	\item The choice of the variational densities
	\item No correllations between variational densities within the CAVI approximation.
	\item Maximum aposteriori estimates via variational are Bayes not scale invariant (see Chaps 2.2 and 1.3.1 of Ref \cite{Beal03variationalalgorithms})
	\item Furhter limitations are summarized in Chap 5 of Ref. \cite{blei2016variational}
	\item see also as reference M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference. Founda-
	tions and Trends in Machine Learning, 1(1-2):1–305, 2008.
	\item Symmetry breaking by KL
	\item overview https://www.coursera.org/learn/bayesian-methods-in-machine-learning/lecture/rSCI2/variational-em-review
	\item (Hierarchical) mixture of experts is an application of EM
\end{itemize}	

\paragraph{On contingency Table}
Take as variational densities Matrices under the constraint of normalization.

\paragraph{The EM Algorithm}. Take as variational densities for the point estimates the dirac distribution. Here the mean field approximation comes out automatically


\section{Applications}
\subsection{Probabilistic latent semantic analysis}
\paragraph{Model}
\paragraph{Inference}
\paragraph{Prediction}
\subsection{Latent Dirichlet allocation}
Speedup for MCMC: "collapsed gibbs sampling"
Extension: "correlated topic models" via logistic normal, or "dynamic topic model" models (see wikipedia).
\paragraph{Model}
\paragraph{Inference}
\paragraph{Prediction}
\section{Markov Chain Monte Carlo}
\paragraph{Note} There has been kind of paradigm change over the last years according Bayesian Methods. Nowadays it is also applied and appreciated for large Datasets via the advent of stochastic variational inference.
\begin{itemize}
	\item Definitions: Stationary Markov Chain, Detailed Balance
	\item Thm: Detailed Balance $\Rightarrow$ Stationary
	\item Gibbs Sampler (is Stationary)
	\item Metropolis Hasties (choice of policy via detailed balance)
	\item Langevin Monte Carlo 
\end{itemize}

\paragraph{Application to Neural Nets}
\begin{itemize}
	\item Langevin Monte Carlo is a canonical way to bayesify weights on neural networks. Just add Noise to the SGD.
	\item Extension: Korattikara, et al. "Baysian dark knowledge"
\end{itemize}

\paragraph{Scalable variational inference}
\begin{itemize}
	\item Definition unbiases estimates
	\item Stochastic generalization of auto encoders
	\item for inference need log-trick or a reparametrization trick \cite{jang2016categorical} and other refs. in order to apply MCMC.
	\item Result has similar structure of Baysian NN (Langevin Monte Carlo) in the sense that it adds noise on the canonical model.
	\item Papers \cite{kingma2013autoencoding}
	\item Videos: https://www.coursera.org/learn/bayesian-methods-in-machine-learning/home/week/5
	\item alternative approaches: via RNN. Do exact factorization and then do RNN on the sequence of conditionals \cite{DBLP:journals/corr/OordKK1}
\end{itemize}
\paragraph{Applications}
\begin{itemize}
	
	\item Generate new data \cite{zhao2016energybased} (gnererate pictures). https://petapixel.com/2016/09/27/neural-photo-editor-like-fully-automatic-photoshop/
	\item Fraud detection / outlier detection
	\item Work with missing data
	\item Data representation / embedding
\end{itemize}

\paragraph{Relation to dropout}
\begin{itemize}
	\item Dropout prevents overfitting as it injects noise during training
	\item Gaussion dropout corresponds to variational Bayesian inference with log-uniform prior.
	\item This allows to construct generalizations of dropout. This eventually leads to learning hyperparameters. This in turn leads to sparse properties, i.e. some weight-components can be omitted (set to zero). This is calles sparse variational dropout. For some applications 99.9 \% of weights can be skipped. This leads to compact NNs. \cite{molchanov2017variational}
	 \item other applications \cite{rezende2015variational, ullrich2017soft}
\end{itemize}

\end{comment}
