\section{Variational Inference}
Variational inference \cite{blei2016variational, Beal03variationalalgorithms} is an alternative to MCMC in the Bayes world. 
In essence, the posterior distributions are approximated by optimizing over a family of testfunctions on a scalar bound, the KL diverence. In the special case where we are interested in the point estimate of of model parameters, this gets more structure and leads \cite{Neal:1999:VEA:308574.308679} to the EM algorithm \cite{Dempster77maximumlikelihood, Gupta:2011:TUE:1969852.1969853}(which also gives an estimate for the hidden variables).  \cite{doi:10.1080/01621459.1990.10476213}.

 For many practical applications the mean field approximation over the variational densities is applied which, leads to coordinate ascent mean-field variational inference (CAVI) \cite{Bishop:2006:PRM:1162264}. CAVI can be seen as "message passing" algorithm and thus connects variational inference with graphical models \cite{Winn:2005:VMP:1046920.1088695, wand_mfvb_2011, NIPS2011_4407, minka2005divergence} (implementation https://dotnet.github.io/infer/). 

From a numerical point of view there have been stochastic generalizations in order to deal with large datasets. In order to avoid computing the functional derivations explicitly for a given model analytically an "automatic differentiation variational inference" (ADVI) method has been proposed \cite{advi_2016}. 

\paragraph{The problem.} Assume that we have random variables $X$ of observed data and $Z$ of latent random variables. Inference of the model $p(x, z) = p(x|z)p(z)$ amounts to finding  the conditional probabilities $p(z|x)$. This quantity is also known posterior. Formally this is trivial via Bayes theorem $p(z | x) = p(x, y) / p(x)$. However computing $p(x) = \int p(x, z) dz$ may be computationally intractable (for example, it may lead to very high dimensional numeric integrations for correlated latent variables). 
Cynically, $p(x)$ is just a normalization factor. In physics it is called partition function and in statistics its called evidence. In fact, computing the evidence is one of the most challenging parts of Bayesian methods.

\paragraph{The optimization problem.}  Variational inference tries to approximately determine  $p(z|x)$ by introducing a family of approximate probability densities $q(z|x) \in \mathcal Q$ and then finding that member that approximates the posterior optimally with respect to the KL divergence, 
\begin{align}
	q^*(z) = \underset{q(z)\in \mathcal Q}{\mathrm{arg min}} 
	~ \kl{q(z)}{p(z|x)}, 
\end{align} 
where the shorthand notation $q(z) \equiv q(z|x)$ was introduced.
This transforms an integration problem to an optimization problem. However, the KL divergence of this form is  of no help to the original problem (computing the marginal $p(x)$) as we still need to compute the term $p(z|x)$ which, requires the term $p(x)$. So we need to find another objective function, the evidence lower bound (ELBO), 
\begin{align} \label{def:elbo}
	\mathcal{F}(q):= \langle \log p(x,z) \rangle _{q(z)} - \langle \log q(z) \rangle _{q(z)}
\end{align}
\begin{theorem}\label{thm:elbo_kl}Let there be given a family of probability densities $\mathcal Q$. Then $q^*(z)$ is a minimizer over $\mathcal Q$ of $\kl{q(z)}{p(z|x)}$ iff it is a maximizer of the ELBO $\mathcal{F}(q)$. 
\end{theorem}
\begin{proof}This follows from
	\begin{align*}
		\kl{q(z)}{p(z|x)}   &=  \langle \log q(z) \rangle _{q(z)} - \langle \log p(z | x) \rangle _{q(z)}   \\     
		  &= - \mathcal{F}(q) + \langle \log p(x) \rangle _{q(z)} \\
		  & = - \mathcal{F}(q)  + p(x), 
	\end{align*}
and that $p(x)$ is constant with respect to $q(z), ~ \delta p(x) / \delta q(z) = 0$
\end{proof}
So, first we need to specify a family of variational densities $\mathcal Q$ and then optimize Equation \ref{def:elbo} over $\mathcal Q$. Not that both, the KL and ELBO objective functions implicitly depend on $x$ and so does $q^*(z)$. This just reflects the dependence on the training data.
\paragraph{Discussion.}Turning to the interpretation of variational inference rewrite
\begin{align}
	\mathcal{F}(q) &= \langle \log p(x | z) \rangle _{q(z)} + \langle \log p(z) \rangle _{q(z)}  - \langle \log q(z) \rangle _{q(z)} \nonumber \\ 
	        &= \langle \log p(x | z) \rangle _{q(z)} - \kl{q(z)}{p(z)}.
\end{align}
The density that maximizes the ELBO thus compromises between  i) putting most probability weight on $z$ where the posterior $p(x|z)$ is large and ii) beeing close to the prior $p(z)$. Furthermore if we assume that the $N$ observed data points are iid of the form $x = x_1, \dots, x_N$  the first expression reads $\langle \log p(x | z) \rangle _{q(z)} = \sum _{i = 1} ^N\langle \log p(x_i | z) \rangle _{q(z)}$. Thus, the first term becomes more important for large number of observed data points. But this is just in line with the usual Bayesian behavior.

Furthermore the ELBO bounds the evidence $p(x)$,
\begin{align}
	p(x) = \mathcal{F}(q) + \kl{q(z)}{p(z|x)} \geq \mathcal{F}(q)
\end{align}
because $D(\cdot) \geq 0$. This property lends ELBO its name and has been used for model selection under the assumption that the ELBO is a good approximation to the marginal likelihood. However, this approach is not rigorous. So we will just use the ELBO as objective function for the optimization problem (which is rigorous).

The KL-divergence enjoys the property $\kl{q}{p} = 0 \Leftrightarrow q=p$. Assume a maximizer $q^*$ of the ELBO, which is also a minimizer of the KL-divergence. Now, if the KL-divergence  $\kl{q^*(z)}{p(z|l)} = 0$ then $q^*$ is the true posterior. However this need not be the case as the variational family typicall suffers from severe restrictions due computational resources. In this case it is not clear how "good" the minimizer of the KL-divergence approximates the posterior \cite{minka2005divergence, Leisink01atighter}.

\subsection{Mean-field approximation}

\begin{definition}[Mean field approximation]
	A family of pdfs $q_Z(z) \in \mathcal Q$ is called mean field approximation if it is of the form $q_Z(z) = \prod _\ell q_{Z_\ell}(z_\ell)$ with  
	$q_{Z_\ell}(z_\ell)\in\mathcal Q_\ell $ and $Q = \otimes_\ell\mathcal Q_\ell$.
\end{definition}
Note that the definition does not assume how the latent variables are factorized. The case where the latent variables are fully factorized is sometimes called \textit{naive mean field approximation} while the case where  some latent variables are collected into groups is sometimes called \textit{generalized mean field approximation}.
In the following we use the shorthand notation $q_\ell(z_\ell) \equiv q_{Z_\ell}(z_\ell)$
\begin{theorem}\label{thm:mf_elbo}
	Given a mean field approximation of a variational family,  $\prod _\ell q_\ell(z_\ell) \in  \otimes_\ell \mathcal Q_\ell$ with $\mathcal Q_\ell=\{q_\ell(z_\ell)\,|\,q_\ell(z_\ell) \text{ is pdf}\,\}$. Then the maximizer of the ELBO is given by
	\begin{align}
		q^*(z) &= \prod _\ell q^*_\ell(z_\ell), \label{eq: mean_field_joint}\\
		q^*(z_\ell) &= \frac{1}{N_\ell} \exp{\langle \log p(x, z_\ell, z_{-\ell}) \rangle_{q_{z_{-\ell}}}}, \label{eq:mean_field_update}
		%N_\ell &= \int dz_\ell \exp{\langle \log p(x, z_\ell, z_{-\ell}) \rangle_{q_{z_{-\ell}}}}, 
	\end{align}
	where $z=\{z_\ell, z_{-\ell} \}$,  $z_{-\ell} = \{z_1, \dots, \cancel{z_\ell} \dots\}$ and $q_{z_{-\ell}} = \prod _{\i\neq \ell} q_i(z_i)$
\end{theorem}
\begin{proof}
	Taking  the functional derivative of the ELBO under the constraint that all $q_\ell(z_\ell)$ are pdfs, i.e., $\int q_\ell(z_\ell)\,dz_\ell = 1$ and  $q_\ell(z_\ell) \geq 0 ~ \forall z_\ell$ via Lagrangian multipliers gives, 
	\begin{align*}
		&\frac{\delta}{\delta q_k(z_k')} \left\{\mathcal{F}(\prod _{\ell=1}^L q^*_\ell(z_\ell)) - \sum_{\ell=1}^L \lambda_\ell\left(\int q_\ell(z_\ell)\,dz_\ell - 1\right)\right\} \\
		= &\frac{\delta}{\delta q_k(z_k')} \left\{
		\int dz_1\, \dots \int dz_1\, p(x, z_1, \dots, z_L) \prod _{\ell =1}^L q_\ell(z_\ell)
		- \sum_{\ell=1}^L \int dz_\ell\, q_\ell(z_\ell) \log q_\ell(z_\ell)\right\} \\ 
		&- \lambda_k \int \delta (z_k - z_k' )\,dz_k \\
		= & \int\dots\int dz_1\dots \cancel{dz_k} \dots dz_L \, p(x, z_1, \dots, z_k', \dots z_L) \prod _{\ell \neq k} q_\ell(z_\ell)  -\log q_k(z_k') -1 - \lambda_k.
	\end{align*}
Solving for $q_k(z_k')$ and using the notation from above gives
\begin{align}
q_k(z_k') = \frac{\exp\langle \log p(x, z'_k, z_{-k}) \rangle_{q_{z_{-k }}}}{\exp(1+\lambda_k)}
\end{align}
The Lagrangian multiplier and thus the is obtained by normalization, $N_k = \exp(1+\lambda_k) = \int dz_k' \, \exp\langle \log p(x, z'_k, z_{-k}) \rangle_{q_{z_{-k }}}$. Together with the functional from this implies that $q_k$ is indeed a pdf.
\end{proof}
Note that (\ref{eq:mean_field_update}) can be equivalently written in terms of complete conditionals $p(z_\ell | x, z_{-\ell})$,
\begin{align} \label{eq:conditional_cavi_update}
	q^*(z_\ell) &= \frac{1}{N_\ell'} \exp{\langle \log p(z_\ell | x, z_{-\ell}) \rangle_{q_{z_{-\ell}}}}, 
\end{align}
because $\langle \log p(x, z_\ell, z_{-\ell}) \rangle_{q_{z_{-\ell}}} = \langle \log p(z_\ell | x, z_{-\ell}) \rangle_{q_{z_{-\ell}}} - \langle \log p(x, z_{-\ell}) \rangle_{q_{z_{-\ell}}}$ and the second term does not depend on $q_\ell(z_\ell)$ and thus may be absorbed into the normalization factor. 
\begin{itemize}
\item  Theorem \ref{thm:elbo_kl} and \ref{thm:mf_elbo} establish the best approximation to the KL-divergence of the true posterior within a factorized family of variational densities. Recall $q(z)\equiv q(z|x)$ and $z=\{z_1,\dots,z_\ell, \dots\}$
\item Integrating out (\ref{eq: mean_field_joint}) shows that the approximation to the marginal posterior corresponds to the variational component, 
\begin{align}
	q^*(z_\ell | x) \equiv  q^*(z_\ell) = q_\ell(z_\ell)
\end{align}
\item   From Theorem \ref{thm:mf_elbo}  $q^*_\ell(z_\ell)$  depends on all other  approximate pdfs of random variables. In practice this yields an iterative scheme of computation, the \textit{coordinate ascent mean-field variational inference algorithm} (CAVI)\cite{blei2016variational, Bishop:2006:PRM:1162264}. Especially from (\ref{eq:conditional_cavi_update}) the connection to the Gibbs sampler in the MCMC approach \cite{doi:10.1080/01621459.1990.10476213, Geman:1984:SRG:2286442.2286617} can be seen: both, CAVI and Gibbs sampling use the complete conditional $p(z_\ell | x, z_{-\ell})$ in their update steps.	
\item  In general this formulation is not very useful for  practical computations. Thus  typically further structure is incorporated by using the exponential family as variational family, which leads to much simpler CAVI update equations and allows variational inference to scale to massive data \cite{blei2016variational, Bernardo03thevariational, MAL-001}.
\end{itemize}
  
\subsection{Point estimates}  
So far there was no interpretation on the latent variables. Consider a model with latent variables $z=\{z,\theta\}$, where $\theta$ are model parameters and $z$ are hidden (e.g., local) variables. Very often $\theta$ is the only quantity of interest, either 1) as a marginal posterior pfd, $\int p(\theta, z|x)\,dz$, or 2) by a point-estimate of it (MAP)  $\operatorname{argmax}_\theta \int p(\theta, z|x)\,dz$, or 3) as a maximum likelihood estimate (MLE)  $\operatorname{argmax}_\theta \int p(x, z|\theta)\,dz$. 

Clearly, the first case can be obtained by making the mean-field ansatz $q(z,\theta) = q(z)q(\theta)$ and using the machinery of Theorem \ref{thm:mf_elbo}.
  
 \begin{comment}
 	

 
 \section{Todo}
 \begin{itemize}
	 \item Stochastic VI - make it applicable to batches (scaling)
 	\item Conjecture MAP as special choice of variational family ($\delta$)
 	\item Conjecture MLE as special choice of variational family ($\delta$) + uniform prior.
 	\item benefit of further restricting to exponential family
 	\item parametrized pdfs and contingency table
 \end{itemize}
\paragraph{Example: Variational Bayes EM}
\paragraph{Limitations of variational inference}
\begin{itemize}
	\item The choice of the variational densities
	\item No correllations between variational densities within the CAVI approximation.
	\item Maximum aposteriori estimates via variational are Bayes not scale invariant (see Chaps 2.2 and 1.3.1 of Ref \cite{Beal03variationalalgorithms})
	\item Furhter limitations are summarized in Chap 5 of Ref. \cite{blei2016variational}
	\item see also as reference M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference. Founda-
	tions and Trends in Machine Learning, 1(1-2):1–305, 2008.
	\item Symmetry breaking by KL
	\item overview https://www.coursera.org/learn/bayesian-methods-in-machine-learning/lecture/rSCI2/variational-em-review
	\item (Hierarchical) mixture of experts is an application of EM
\end{itemize}	

\paragraph{On contingency Table}
Take as variational densities Matrices under the constraint of normalization.

\paragraph{The EM Algorithm}. Take as variational densities for the point estimates the dirac distribution. Here the mean field approximation comes out automatically


\section{Applications}
\subsection{Probabilistic latent semantic analysis}
\paragraph{Model}
\paragraph{Inference}
\paragraph{Prediction}
\subsection{Latent Dirichlet allocation}
Speedup for MCMC: "collapsed gibbs sampling"
Extension: "correlated topic models" via logistic normal, or "dynamic topic model" models (see wikipedia).
\paragraph{Model}
\paragraph{Inference}
\paragraph{Prediction}
\section{Markov Chain Monte Carlo}
\paragraph{Note} There has been kind of paradigm change over the last years according Bayesian Methods. Nowadays it is also applied and appreciated for large Datasets via the advent of stochastic variational inference.
\begin{itemize}
	\item Definitions: Stationary Markov Chain, Detailed Balance
	\item Thm: Detailed Balance $\Rightarrow$ Stationary
	\item Gibbs Sampler (is Stationary)
	\item Metropolis Hasties (choice of policy via detailed balance)
	\item Langevin Monte Carlo 
\end{itemize}

\paragraph{Application to Neural Nets}
\begin{itemize}
	\item Langevin Monte Carlo is a canonical way to bayesify weights on neural networks. Just add Noise to the SGD.
	\item Extension: Korattikara, et al. "Baysian dark knowledge"
\end{itemize}

\paragraph{Scalable variational inference}
\begin{itemize}
	\item Definition unbiases estimates
	\item Stochastic generalization of auto encoders
	\item for inference need log-trick or a reparametrization trick \cite{jang2016categorical} and other refs. in order to apply MCMC.
	\item Result has similar structure of Baysian NN (Langevin Monte Carlo) in the sense that it adds noise on the canonical model.
	\item Papers \cite{kingma2013autoencoding}
	\item Videos: https://www.coursera.org/learn/bayesian-methods-in-machine-learning/home/week/5
	\item alternative approaches: via RNN. Do exact factorization and then do RNN on the sequence of conditionals \cite{DBLP:journals/corr/OordKK1}
\end{itemize}
\paragraph{Applications}
\begin{itemize}
	
	\item Generate new data \cite{zhao2016energybased} (gnererate pictures). https://petapixel.com/2016/09/27/neural-photo-editor-like-fully-automatic-photoshop/
	\item Fraud detection / outlier detection
	\item Work with missing data
	\item Data representation / embedding
\end{itemize}

\paragraph{Relation to dropout}
\begin{itemize}
	\item Dropout prevents overfitting as it injects noise during training
	\item Gaussion dropout corresponds to variational Bayesian inference with log-uniform prior.
	\item This allows to construct generalizations of dropout. This eventually leads to learning hyperparameters. This in turn leads to sparse properties, i.e. some weight-components can be omitted (set to zero). This is calles sparse variational dropout. For some applications 99.9 \% of weights can be skipped. This leads to compact NNs. \cite{molchanov2017variational}
	 \item other applications \cite{rezende2015variational, ullrich2017soft}
\end{itemize}

\end{comment}
