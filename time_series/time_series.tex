\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{cancel}
\usetikzlibrary{arrows}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{verbatim}

\usepackage{mathtools}

\DeclarePairedDelimiterX{\klx}[2]{(}{)}{%
	#1\;\delimsize\|\;#2%
}
%\newcommand{\kl}{\mathrm{KL}\klx}
\newcommand{\kl}{D\klx}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

\begin{document}
\bibliographystyle{plain}
\title{Time Series}
\author{Viktor}
\maketitle
\paragraph{Introduction.}
Time series are commonly thought as a superposition of several components
\begin{itemize}
	\item \textit{Trend}: long-term direction of time series
	\item \textit{Seasonal}: pattern that repeats with known and fixed periodicity 
	\item \textit{Cycle}: pattern that repeats but with unknown und changing periodicity.
	\item \textit{Autocorrelation}:
	\item \textit{Error}: unpredictable component of time series.
\end{itemize}
The aim to time series modelling is to capture all effects until only error (=noise) is left. More precisely if there ist still come uncaptured trend, seasonal, cylce or autocorrelation then we can be pretty sure that we can improve the model. However, this does not imply that a model that just has unexplained white noise cannot be improved.
 
\paragraph{Model diagnostics.}
The autocorrelation function (ACF) is the Pearson correlation of a time dependent random variable $Y_t$ with itself shifted in time by $\tau$.
\begin{align}
	\rho(\tau) =  \frac {\sum_{t=\tau+1} ^ T  \left(y_t - \bar y\right) \left(y_{t-\tau} - \bar y\right) }{\sum_{t=1} ^ T (y_t - \bar y)^2 }
\end{align}
The autocorrelation function depends on $\tau$. For white noise $95\,\%$ of its ($\tau$-dependent) values are within the interval $\pm2/\sqrt{T}$. Thus the  and is used to identify white noise. 
\cite{hyndman_forecasting_principles_2018}

\begin{definition}[Evaluation metrics]
	Let $y_t$ be the observed time series and  $\hat y_t$ the corresponding predictions of $T$ timesteps.
	\begin{itemize}
		\item Mean absolute error:  $\frac{1}{T}\sum_{t=1}^T \left | y_t - \hat y_t\right |$
		  \item Mean squared error: $\frac{1}{T}\sum_{t=1}^T \left ( y_t - \hat y_t\right ) ^2$. 
		  \item Root mean squared error:  $\sqrt{\frac{1}{T}\sum_{t=1}^T \left ( y_t - \hat y_t\right ) ^2}$
		  \item Mean absolute percentage error
		  \item Symmetric mean absolute percentage error
		  \item Huber loss: 
		  \item $\rho$-risk metric:
		  
	\end{itemize}
\end{definition}


\paragraph{Overview of methods} 
\begin{itemize}
	\item Regression models
	\item Exponential smoothing methods
	\item Arima
	\item Dynamic Regression 
	\item Hierarchical forecasting
	\item Vector autoregression
	\item Neural networks
	\item survival analysis
\end{itemize}

\paragraph{Transformations.}


\bibliography{papers}
\end{document}
